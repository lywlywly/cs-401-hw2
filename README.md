# Project 2: DevOps and Cloud Computing

## Introduction

In this project, I developed a machine learning-based playlist recommender system using Frequent Itemset Mining. The system includes a REST server and client, and it is deployed using Kubernetes.

## Software Components

### Playlist Rules Generator

The ML Processor module is responsible for running a Frequent Itemset Mining algorithm to generate rules for song recommendations. It downloads the dataset from the `DATASET_URL` environment variable and saves the generated rules in the directory specified by the `DATA_DIR` environment variable.

### REST API Server

The REST API Server exposes a POST endpoint at `/api/recommend` on port 52003. It accepts a request containing a list of songs and responds with song recommendations based on the input.

Request:

```json
{
    "songs": [
        "HUMBLE.",
        "No Role Modelz"
    ]
}
```

Response:

```jsonc
{
    "songs": [
        "goosebumps",
        "Bad and Boujee (feat. Lil Uzi Vert)",
        "Broccoli (feat. Lil Yachty)",
    ],
    "model_date": "2024-02-21 12:06:07.238686",
    "version": "1.0"
}
```

The server reads the rules generated by the ML Processor from the directory specified by the `DATA_DIR` environment variable and provides responses with recommended songs.

### REST API Client
x
The REST API client, implemented in `client.py`, allows users to make requests to the REST API server with an arbitrary number of songs. The songs are passed as command-line arguments using the `-i` flag.

Example usage:

```sh
$ python3 client.py -i "DNA." "No Role Modelz"
Code version: 1.0
Model time: 2024-02-21 12:06:07.238686
Recommended songs: ['XO TOUR Llif3', 'Bad and Boujee (feat. Lil Uzi Vert)', 'Mask Off', 'HUMBLE.']
```

## Continuous Integration and Continuous Delivery

### Docker Containers

Two Docker containers are created:

- ML container: This container runs the ML Processor module to generate recommendation rules and save them. It mounts the a volume and saves generated rules to the directory specified by `DATA_DIR` environment.
- REST server container: This container runs the REST API Server. It reads the rules generated by the ML Processor from the same volume and serves HTTP requests. It listens on port 9999 inside the container and maps it to port 52002 on the host.

The containers can be run using the following commands, assuming the `hw2` volume is already created:

```shell
$ docker run --mount type=volume,src=hw2,target=/data -e DATA_DIR=/data ml
$ docker run -p 127.0.0.1:52002:9999 --mount type=volume,src=hw2,target=/data -e DATA_DIR=/data server
```

### Kubernetes Deployment and Service

The Kubernetes configurations are defined in the `k8s.yml` file. The configurations include:

- A persistent volume claim (PVC) to provide storage for the ML Processor and REST API Server containers.
- A deployment that runs the ML Processor container and mounts the PVC at `/data`.
- A deployment that runs the REST API Server container and mounts the PVC at `/data`. It listens on port 9999 inside the container.
- A service that exposes the REST API Server container's port 9999 on port 52003 of the VM.
